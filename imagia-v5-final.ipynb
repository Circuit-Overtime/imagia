{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-22T19:51:40.734471Z",
     "iopub.status.busy": "2025-02-22T19:51:40.734252Z",
     "iopub.status.idle": "2025-02-22T21:37:00.217653Z",
     "shell.execute_reply": "2025-02-22T21:37:00.216943Z",
     "shell.execute_reply.started": "2025-02-22T19:51:40.734451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743674006e3e4a92a077233a10e00042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b04643d6fd452c97a348985b0afd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d01d5a52224f359b93f4aa011b684e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8315e9c1e4421d8cb3fd88f75da380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb3270fd0dd4bfc858854e38c68d20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7855b0e5554d43d58e30b06bae28b025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526a36e09859467f83fc0e0e4bb5df64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2967f34d07e430d93f13a1949bb890a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1611 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed59cb44b7c84aa7a5f44201ed846a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0076f26a34974419b03cc41d30c23c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtreysarkar\u001b[0m (\u001b[33mtreysarkar-auk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250222_195230-990pqhiu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/treysarkar-auk/imagia-finetune-v3/runs/990pqhiu' target=\"_blank\">imagiav3</a></strong> to <a href='https://wandb.ai/treysarkar-auk/imagia-finetune-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/treysarkar-auk/imagia-finetune-v3' target=\"_blank\">https://wandb.ai/treysarkar-auk/imagia-finetune-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/treysarkar-auk/imagia-finetune-v3/runs/990pqhiu' target=\"_blank\">https://wandb.ai/treysarkar-auk/imagia-finetune-v3/runs/990pqhiu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4535' max='4535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4535/4535 1:43:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.448600</td>\n",
       "      <td>1.396306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.428800</td>\n",
       "      <td>1.350275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.398700</td>\n",
       "      <td>1.328038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.402300</td>\n",
       "      <td>1.316131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.389900</td>\n",
       "      <td>1.310221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model uploaded to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (Uncomment if needed)\n",
    "# !pip install transformers datasets torch huggingface_hub\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    ")\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "torch.utils.backcompat.broadcast_warning.enabled = False  # Disable PyTorch warnings\n",
    "\n",
    "import warnings\n",
    "import wandb\n",
    "\n",
    "# Suppress specific FutureWarnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "HF_TOKEN = \"hf_xxxxxxx\"  # Replace with your HF token\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"treysarkar/PromptPimpSynth\")\n",
    "\n",
    "# Split 90% train / 10% validation\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(int(0.9 * len(dataset[\"train\"])))), \n",
    "    \"validation\": dataset[\"train\"].shuffle(seed=42).select(range(int(0.9 * len(dataset[\"train\"])), len(dataset[\"train\"])))\n",
    "})\n",
    "\n",
    "# Load tokenizer\n",
    "MODEL_NAME = \"google-t5/t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"short_prompt\"], max_length=256, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        examples[\"long_prompt\"], max_length=256, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"short_prompt\", \"long_prompt\"])\n",
    "\n",
    "# Let us now load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "wandb.login(key=\"5xxxxxxx\") # wandB for tracking in visual fashionm\n",
    "wandb.init(project=\"imagia-finetune-v3\", name=\"imagiav3\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=\"T5-Imagia-runv2\",\n",
    "    output_dir=\"./T5-finetuned\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=800,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,  \n",
    "    hub_model_id=\"treysarkar/T5-PromptPimp-Finetunedv2\",  \n",
    "    hub_token=HF_TOKEN,  \n",
    "    report_to=\"wandb\"\n",
    "    \n",
    ")\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model locally\n",
    "trainer.save_model(\"./T5-finetuned\")\n",
    "tokenizer.save_pretrained(\"./T5-finetuned\")\n",
    "\n",
    "\n",
    "trainer.push_to_hub()\n",
    "\n",
    "print(\"Training completed! Model uploaded to Hugging Face.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T21:49:34.510173Z",
     "iopub.status.busy": "2025-02-22T21:49:34.509813Z",
     "iopub.status.idle": "2025-02-22T21:50:05.492985Z",
     "shell.execute_reply": "2025-02-22T21:50:05.492360Z",
     "shell.execute_reply.started": "2025-02-22T21:49:34.510140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Imagine please\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sky fills with cirrostratus clouds, sun sets behind dark triangle cloud.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Output:\n",
      "\n",
      "A close up view of a large cirrostratus cloud that is filled with dark circles. The sun is setting behind the cirrostratus cloud. The sky is filled with dark circles that are shaped like triangles\n",
      "\n",
      "\n",
      "\n",
      "Imagine please\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TextStreamer\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"treysarkar/T5-PromptPimp-Finetunedv2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Automatically detect device\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "\n",
    "# For the ChatGPT ahhhhhh style output\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "while True:  # Infinite loop\n",
    "    print(\"\\n\\n\")\n",
    "    text = \"Imagine please\\n\"\n",
    "\n",
    "    # Animated typing effect\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    prmpt = input(\"\")  # User enters prompt\n",
    "\n",
    "    if prmpt.strip().lower() == \"exit\":  # Escape condition\n",
    "        break\n",
    "\n",
    "    # Tokenize input and move tensors to the correct device\n",
    "    input_ids = tokenizer(prmpt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Generate response with streaming\n",
    "    print(\"\\nModel Output:\\n\")\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=50,  \n",
    "            streamer=streamer  \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
